<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Feb 15th Journal Club | Research Blog</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Feb 15th Journal Club" />
<meta name="author" content="Jae Eun Park (dawn2089@snu.ac.kr)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Themes: NLP, debiasing, gender bias, anthropomorphism, non-verbal cues, virtual assistants Presenters: JP (Jae Eun Park), HM (Hoyoung Maeng) Topic 1: NLP, debiasing, gender bias[JP] 선정 이유 자연어처리 분야에서 사용되는 debiasing method에 대해 정리해보고자 함 내용 요약 NLP에서 나타나는 gender bias는 allocation bias와 representation bias로 나눌 수 있다. allocation bias는 시스템이 특정 집단에 대해 자원을 불공정하게 분배하거나 성능이 더 좋지 못한 것을 얘기하며, representation bias는 특정 집단과 개념 간의 관계가 워드임베딩이나 모델 파라미터에 반영되는 것을 말한다. gender bias를 denigration, stereotyping, recognition, under-representation 등으로 나눌 수도 있다. 이러한 bias는 학습 데이터에서 나타나는 패턴으로 인해 야기된다. Gender bias를 측정하기 위해서 IAT에서 착안한 WEAT(Word Embedding Association Test)를 사용하고, word embedding의 gender subspace를 분석하거나 gender-swapping 방법을 이용해서 gender에 따른 시스템의 성능을 측정한다. 이러한 metric으로 False Positive Equality Difference(FPED)나 False Negative Equality Difference(FNED) 등이 있다. Debiasing method는 크게 text corpora를 debias하는 방법과 prediction algorithm을 조정하는 방법이 있다. 이 중 text corpora를 debias하는 방법에는 retraining과 inference가 있다. retraining은 gender bias를 초기에 잡아낼 수 있지만 새로운 데이터가 필요하다는 점에서 비용이 많이 든다는 단점이 있으며, inference는 기존 모델을 사용하므로 비용을 줄일 수 있다는 장점이 있다. Training Corpora를 debias하는 방법으로는 data augmentation, gender tagging, bias fine-tuning 등이 있다. Data augmentation은 gender swapping 방법을 이용해서 학습 데이터의 양을 늘리는 방법이다. 이 방법을 사용하면 한 성별에 치우친 데이터의 균형을 잡을 수 있다. Gender tagging은 성별이 나타나있지 않은 대명사가 남성 등 데이터가 많은 그룹으로 분류되는 것을 막기 위해 training data에서 성별을 태깅하는 방법을 말한다. Bias fine-tuning의 경우 unbiased dataset에서 transfer learning을 먼저 한 다음 좀 더 biased된 데이터셋을 학습시킨다. Word embedding을 debias하는 방법에는 gender subspace를 제거하는 방법과 gender-neutral word embedding을 학습시키는 방법이 있다. 알고리즘을 debias하는 방법의 경우 prediction을 constrain하는 방법이 있는데, NLP 모델이 stereotypic한 방법으로 예측하는 정도를 제한하는 것이다. 또한 Adversarial learning을 활용하여 discriminator가 gender를 알아내는 것을 generator가 방지하는 방식으로 학습을 시킬 수 있다. 장단점 장점: Gender bias에 대한 다양한 debiasing method가 잘 정리되어 있음. 단점: 각 방법의 효능과 방법 간 비교가 더 자세히 나와있었으면 좋을 듯함. Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... &amp; Wang, W. Y. (2019). Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976. Topic 2: Anthropomorphism, non-verbal cues, Virtual Assistants [HM] 선정이유 인공지능 에이전트에 대하여 non-verbal cues 유무에 따른 실험을 진행한 논문을 스터디 하고 싶었음. 또한 인공지능 보이스에 대하여 실험참가자들이 어떻게 인식하는지 학습을 위하여 선정함. 내용요약 인공지능 에이전트에 대하여 Anthropomorphism과 non-verbal cues의 역할을 확인하는 실험 논문이였음. 인공지능 스피커(Smart Speaker, SS), 얼굴 형태만 있는 로봇(AnthropoMorphic Robot, AMR), 얼굴 형태에 눈이 움직이는 로봇(AnthropoMorphic Social Robot, AMSR) 3가지 에이전트에 대하여, 실험참가자와 에이전트는 서로 대화를 주고 받을 수 있고 음식을 만드는 환경에서 에이전트가 요리 레시피를 제시하는 실험을 진행하였다. 실험에 대한 평가 지표로는 embodiment and social eye-gaze 이다. 이후 Quantitative 평가방법으로, co-presence, gaze to agent, conversational turns를 측정하여, 모든 측면에서 SS &lt; AMR &lt; AMSR 순위로 결과가 나타났다. 그리고 에이전트에 intelligent 를 설문하였을때는 Anthropomorphism과 non-verbal cues 관계 없이 모두 동일한 intelligent 를 느꼈다고 답하였다. 반면 Qualitative 평가를 실시한 결과에는 주제에 따라 오히려 social cues가 없을때 더 선호된다는 결론을 얻었다. 장단점 인공지능 에이전트 사용에 있어서 현실에서 고려할만한 경우들에 대하여 가설검증을 통해 확인한 점이 장점이라고 생각한다. 또한 실험 후 qualitative measures를 통해 실험 참가자들의 의견을 확인 할 수 있어서 좋았다. 그러나 요리하는 한가지 상황에 대해서만 측정하여 다른 환경에서는 어떤 결과가 나올지 궁금한 측면이 있다. 의의 인공지능 에이전트와 소통하는 과정에서 non-verbal cues의 영향을 확인할 수 있었으며, 학습 주제별로 적절한 사용이 필요함을 확인할 수 있었다. Kontogiorgos, D., Pereira, A., Andersson, O., Koivisto, M., Rabal, E. G., Vartiainen, V., &amp; Gustafson, J. (2019). The effects of anthropomorphism and non-verbal social behaviour in virtual assistants. IVA 2019 - Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, 133–140." />
<meta property="og:description" content="Themes: NLP, debiasing, gender bias, anthropomorphism, non-verbal cues, virtual assistants Presenters: JP (Jae Eun Park), HM (Hoyoung Maeng) Topic 1: NLP, debiasing, gender bias[JP] 선정 이유 자연어처리 분야에서 사용되는 debiasing method에 대해 정리해보고자 함 내용 요약 NLP에서 나타나는 gender bias는 allocation bias와 representation bias로 나눌 수 있다. allocation bias는 시스템이 특정 집단에 대해 자원을 불공정하게 분배하거나 성능이 더 좋지 못한 것을 얘기하며, representation bias는 특정 집단과 개념 간의 관계가 워드임베딩이나 모델 파라미터에 반영되는 것을 말한다. gender bias를 denigration, stereotyping, recognition, under-representation 등으로 나눌 수도 있다. 이러한 bias는 학습 데이터에서 나타나는 패턴으로 인해 야기된다. Gender bias를 측정하기 위해서 IAT에서 착안한 WEAT(Word Embedding Association Test)를 사용하고, word embedding의 gender subspace를 분석하거나 gender-swapping 방법을 이용해서 gender에 따른 시스템의 성능을 측정한다. 이러한 metric으로 False Positive Equality Difference(FPED)나 False Negative Equality Difference(FNED) 등이 있다. Debiasing method는 크게 text corpora를 debias하는 방법과 prediction algorithm을 조정하는 방법이 있다. 이 중 text corpora를 debias하는 방법에는 retraining과 inference가 있다. retraining은 gender bias를 초기에 잡아낼 수 있지만 새로운 데이터가 필요하다는 점에서 비용이 많이 든다는 단점이 있으며, inference는 기존 모델을 사용하므로 비용을 줄일 수 있다는 장점이 있다. Training Corpora를 debias하는 방법으로는 data augmentation, gender tagging, bias fine-tuning 등이 있다. Data augmentation은 gender swapping 방법을 이용해서 학습 데이터의 양을 늘리는 방법이다. 이 방법을 사용하면 한 성별에 치우친 데이터의 균형을 잡을 수 있다. Gender tagging은 성별이 나타나있지 않은 대명사가 남성 등 데이터가 많은 그룹으로 분류되는 것을 막기 위해 training data에서 성별을 태깅하는 방법을 말한다. Bias fine-tuning의 경우 unbiased dataset에서 transfer learning을 먼저 한 다음 좀 더 biased된 데이터셋을 학습시킨다. Word embedding을 debias하는 방법에는 gender subspace를 제거하는 방법과 gender-neutral word embedding을 학습시키는 방법이 있다. 알고리즘을 debias하는 방법의 경우 prediction을 constrain하는 방법이 있는데, NLP 모델이 stereotypic한 방법으로 예측하는 정도를 제한하는 것이다. 또한 Adversarial learning을 활용하여 discriminator가 gender를 알아내는 것을 generator가 방지하는 방식으로 학습을 시킬 수 있다. 장단점 장점: Gender bias에 대한 다양한 debiasing method가 잘 정리되어 있음. 단점: 각 방법의 효능과 방법 간 비교가 더 자세히 나와있었으면 좋을 듯함. Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... &amp; Wang, W. Y. (2019). Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976. Topic 2: Anthropomorphism, non-verbal cues, Virtual Assistants [HM] 선정이유 인공지능 에이전트에 대하여 non-verbal cues 유무에 따른 실험을 진행한 논문을 스터디 하고 싶었음. 또한 인공지능 보이스에 대하여 실험참가자들이 어떻게 인식하는지 학습을 위하여 선정함. 내용요약 인공지능 에이전트에 대하여 Anthropomorphism과 non-verbal cues의 역할을 확인하는 실험 논문이였음. 인공지능 스피커(Smart Speaker, SS), 얼굴 형태만 있는 로봇(AnthropoMorphic Robot, AMR), 얼굴 형태에 눈이 움직이는 로봇(AnthropoMorphic Social Robot, AMSR) 3가지 에이전트에 대하여, 실험참가자와 에이전트는 서로 대화를 주고 받을 수 있고 음식을 만드는 환경에서 에이전트가 요리 레시피를 제시하는 실험을 진행하였다. 실험에 대한 평가 지표로는 embodiment and social eye-gaze 이다. 이후 Quantitative 평가방법으로, co-presence, gaze to agent, conversational turns를 측정하여, 모든 측면에서 SS &lt; AMR &lt; AMSR 순위로 결과가 나타났다. 그리고 에이전트에 intelligent 를 설문하였을때는 Anthropomorphism과 non-verbal cues 관계 없이 모두 동일한 intelligent 를 느꼈다고 답하였다. 반면 Qualitative 평가를 실시한 결과에는 주제에 따라 오히려 social cues가 없을때 더 선호된다는 결론을 얻었다. 장단점 인공지능 에이전트 사용에 있어서 현실에서 고려할만한 경우들에 대하여 가설검증을 통해 확인한 점이 장점이라고 생각한다. 또한 실험 후 qualitative measures를 통해 실험 참가자들의 의견을 확인 할 수 있어서 좋았다. 그러나 요리하는 한가지 상황에 대해서만 측정하여 다른 환경에서는 어떤 결과가 나올지 궁금한 측면이 있다. 의의 인공지능 에이전트와 소통하는 과정에서 non-verbal cues의 영향을 확인할 수 있었으며, 학습 주제별로 적절한 사용이 필요함을 확인할 수 있었다. Kontogiorgos, D., Pereira, A., Andersson, O., Koivisto, M., Rabal, E. G., Vartiainen, V., &amp; Gustafson, J. (2019). The effects of anthropomorphism and non-verbal social behaviour in virtual assistants. IVA 2019 - Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, 133–140." />
<link rel="canonical" href="http://localhost:4000/blog/2021-02-15/study" />
<meta property="og:url" content="http://localhost:4000/blog/2021-02-15/study" />
<meta property="og:site_name" content="Research Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-15T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Feb 15th Journal Club" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jae Eun Park (dawn2089@snu.ac.kr)"},"dateModified":"2021-02-15T00:00:00+09:00","datePublished":"2021-02-15T00:00:00+09:00","description":"Themes: NLP, debiasing, gender bias, anthropomorphism, non-verbal cues, virtual assistants Presenters: JP (Jae Eun Park), HM (Hoyoung Maeng) Topic 1: NLP, debiasing, gender bias[JP] 선정 이유 자연어처리 분야에서 사용되는 debiasing method에 대해 정리해보고자 함 내용 요약 NLP에서 나타나는 gender bias는 allocation bias와 representation bias로 나눌 수 있다. allocation bias는 시스템이 특정 집단에 대해 자원을 불공정하게 분배하거나 성능이 더 좋지 못한 것을 얘기하며, representation bias는 특정 집단과 개념 간의 관계가 워드임베딩이나 모델 파라미터에 반영되는 것을 말한다. gender bias를 denigration, stereotyping, recognition, under-representation 등으로 나눌 수도 있다. 이러한 bias는 학습 데이터에서 나타나는 패턴으로 인해 야기된다. Gender bias를 측정하기 위해서 IAT에서 착안한 WEAT(Word Embedding Association Test)를 사용하고, word embedding의 gender subspace를 분석하거나 gender-swapping 방법을 이용해서 gender에 따른 시스템의 성능을 측정한다. 이러한 metric으로 False Positive Equality Difference(FPED)나 False Negative Equality Difference(FNED) 등이 있다. Debiasing method는 크게 text corpora를 debias하는 방법과 prediction algorithm을 조정하는 방법이 있다. 이 중 text corpora를 debias하는 방법에는 retraining과 inference가 있다. retraining은 gender bias를 초기에 잡아낼 수 있지만 새로운 데이터가 필요하다는 점에서 비용이 많이 든다는 단점이 있으며, inference는 기존 모델을 사용하므로 비용을 줄일 수 있다는 장점이 있다. Training Corpora를 debias하는 방법으로는 data augmentation, gender tagging, bias fine-tuning 등이 있다. Data augmentation은 gender swapping 방법을 이용해서 학습 데이터의 양을 늘리는 방법이다. 이 방법을 사용하면 한 성별에 치우친 데이터의 균형을 잡을 수 있다. Gender tagging은 성별이 나타나있지 않은 대명사가 남성 등 데이터가 많은 그룹으로 분류되는 것을 막기 위해 training data에서 성별을 태깅하는 방법을 말한다. Bias fine-tuning의 경우 unbiased dataset에서 transfer learning을 먼저 한 다음 좀 더 biased된 데이터셋을 학습시킨다. Word embedding을 debias하는 방법에는 gender subspace를 제거하는 방법과 gender-neutral word embedding을 학습시키는 방법이 있다. 알고리즘을 debias하는 방법의 경우 prediction을 constrain하는 방법이 있는데, NLP 모델이 stereotypic한 방법으로 예측하는 정도를 제한하는 것이다. 또한 Adversarial learning을 활용하여 discriminator가 gender를 알아내는 것을 generator가 방지하는 방식으로 학습을 시킬 수 있다. 장단점 장점: Gender bias에 대한 다양한 debiasing method가 잘 정리되어 있음. 단점: 각 방법의 효능과 방법 간 비교가 더 자세히 나와있었으면 좋을 듯함. Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... &amp; Wang, W. Y. (2019). Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976. Topic 2: Anthropomorphism, non-verbal cues, Virtual Assistants [HM] 선정이유 인공지능 에이전트에 대하여 non-verbal cues 유무에 따른 실험을 진행한 논문을 스터디 하고 싶었음. 또한 인공지능 보이스에 대하여 실험참가자들이 어떻게 인식하는지 학습을 위하여 선정함. 내용요약 인공지능 에이전트에 대하여 Anthropomorphism과 non-verbal cues의 역할을 확인하는 실험 논문이였음. 인공지능 스피커(Smart Speaker, SS), 얼굴 형태만 있는 로봇(AnthropoMorphic Robot, AMR), 얼굴 형태에 눈이 움직이는 로봇(AnthropoMorphic Social Robot, AMSR) 3가지 에이전트에 대하여, 실험참가자와 에이전트는 서로 대화를 주고 받을 수 있고 음식을 만드는 환경에서 에이전트가 요리 레시피를 제시하는 실험을 진행하였다. 실험에 대한 평가 지표로는 embodiment and social eye-gaze 이다. 이후 Quantitative 평가방법으로, co-presence, gaze to agent, conversational turns를 측정하여, 모든 측면에서 SS &lt; AMR &lt; AMSR 순위로 결과가 나타났다. 그리고 에이전트에 intelligent 를 설문하였을때는 Anthropomorphism과 non-verbal cues 관계 없이 모두 동일한 intelligent 를 느꼈다고 답하였다. 반면 Qualitative 평가를 실시한 결과에는 주제에 따라 오히려 social cues가 없을때 더 선호된다는 결론을 얻었다. 장단점 인공지능 에이전트 사용에 있어서 현실에서 고려할만한 경우들에 대하여 가설검증을 통해 확인한 점이 장점이라고 생각한다. 또한 실험 후 qualitative measures를 통해 실험 참가자들의 의견을 확인 할 수 있어서 좋았다. 그러나 요리하는 한가지 상황에 대해서만 측정하여 다른 환경에서는 어떤 결과가 나올지 궁금한 측면이 있다. 의의 인공지능 에이전트와 소통하는 과정에서 non-verbal cues의 영향을 확인할 수 있었으며, 학습 주제별로 적절한 사용이 필요함을 확인할 수 있었다. Kontogiorgos, D., Pereira, A., Andersson, O., Koivisto, M., Rabal, E. G., Vartiainen, V., &amp; Gustafson, J. (2019). The effects of anthropomorphism and non-verbal social behaviour in virtual assistants. IVA 2019 - Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, 133–140.","headline":"Feb 15th Journal Club","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2021-02-15/study"},"url":"http://localhost:4000/blog/2021-02-15/study"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/assets/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Research Blog" />

  <!-- Google Analytics-->
  
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/blog/">
      <h2 class="nav-title">Research Blog</h2>
    </a>
    <ul>
      <li><a href="/blog/about">About</a></li>
      <li><a href="/blog/">Posts</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Jae Eun Park (dawn2089@snu.ac.kr)
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2021-02-15 00:00:00 +0900">February 15, 2021</time>
    
  </div>

  <h1 class="post-title">Feb 15th Journal Club</h1>
  <div class="post-line"></div>

  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Themes: NLP, debiasing, gender bias, anthropomorphism, non-verbal cues, virtual assistants
</code></pre></div></div>

<p>Presenters: JP (Jae Eun Park), HM (Hoyoung Maeng)<br /></p>

<hr />

<h2 id="topic-1-nlp-debiasing-gender-biasjp">Topic 1: NLP, debiasing, gender bias[JP]</h2>

<p><strong>선정 이유</strong><br />
자연어처리 분야에서 사용되는 debiasing method에 대해 정리해보고자 함<br /></p>

<p><strong>내용 요약</strong><br />
NLP에서 나타나는 gender bias는 allocation bias와 representation bias로 나눌 수 있다. allocation bias는 시스템이 특정 집단에 대해 자원을 불공정하게 분배하거나 성능이 더 좋지 못한 것을 얘기하며, representation bias는 특정 집단과 개념 간의 관계가 워드임베딩이나 모델 파라미터에 반영되는 것을 말한다. gender bias를 denigration, stereotyping, recognition, under-representation 등으로 나눌 수도 있다. 이러한 bias는 학습 데이터에서 나타나는 패턴으로 인해 야기된다. Gender bias를 측정하기 위해서 IAT에서 착안한 WEAT(Word Embedding Association Test)를 사용하고, word embedding의 gender subspace를 분석하거나 gender-swapping 방법을 이용해서 gender에 따른 시스템의 성능을 측정한다. 이러한 metric으로 False Positive Equality Difference(FPED)나 False Negative Equality Difference(FNED) 등이 있다. <br /></p>

<p>Debiasing method는 크게 text corpora를 debias하는 방법과 prediction algorithm을 조정하는 방법이 있다. 이 중 text corpora를 debias하는 방법에는 retraining과 inference가 있다. retraining은 gender bias를 초기에 잡아낼 수 있지만 새로운 데이터가 필요하다는 점에서 비용이 많이 든다는 단점이 있으며, inference는 기존 모델을 사용하므로 비용을 줄일 수 있다는 장점이 있다. Training Corpora를 debias하는 방법으로는 data augmentation, gender tagging, bias fine-tuning 등이 있다. Data augmentation은 gender swapping 방법을 이용해서 학습 데이터의 양을 늘리는 방법이다. 이 방법을 사용하면 한 성별에 치우친 데이터의 균형을 잡을 수 있다. Gender tagging은 성별이 나타나있지 않은 대명사가 남성 등 데이터가 많은 그룹으로 분류되는 것을 막기 위해 training data에서 성별을 태깅하는 방법을 말한다. Bias fine-tuning의 경우 unbiased dataset에서 transfer learning을 먼저 한 다음 좀 더 biased된 데이터셋을 학습시킨다. Word embedding을 debias하는 방법에는 gender subspace를 제거하는 방법과 gender-neutral word embedding을 학습시키는 방법이 있다. 알고리즘을 debias하는 방법의 경우 prediction을 constrain하는 방법이 있는데, NLP 모델이 stereotypic한 방법으로 예측하는 정도를 제한하는 것이다. 또한 Adversarial learning을 활용하여 discriminator가 gender를 알아내는 것을 generator가 방지하는 방식으로 학습을 시킬 수 있다.<br /></p>

<p><strong>장단점</strong><br />
장점: Gender bias에 대한 다양한 debiasing method가 잘 정리되어 있음.<br />
단점: 각 방법의 효능과 방법 간 비교가 더 자세히 나와있었으면 좋을 듯함. <br /></p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... &amp; Wang, W. Y. (2019). Mitigating gender bias in natural language processing: Literature review. arXiv preprint arXiv:1906.08976.
</code></pre></div></div>

<p><br /></p>

<h2 id="topic-2-anthropomorphism-non-verbal-cues-virtual-assistants-hm">Topic 2: Anthropomorphism, non-verbal cues, Virtual Assistants [HM]</h2>

<p><strong>선정이유</strong><br />
인공지능 에이전트에 대하여 non-verbal cues 유무에 따른 실험을 진행한 논문을 스터디 하고 싶었음. 또한 인공지능 보이스에 대하여 실험참가자들이 어떻게 인식하는지 학습을 위하여 선정함.<br /></p>

<p><strong>내용요약</strong> <br /> 
인공지능 에이전트에 대하여 Anthropomorphism과 non-verbal cues의 역할을 확인하는 실험 논문이였음. 인공지능 스피커(Smart Speaker, SS), 얼굴 형태만 있는 로봇(AnthropoMorphic Robot, AMR), 얼굴 형태에 눈이 움직이는 로봇(AnthropoMorphic Social Robot, AMSR) 3가지 에이전트에 대하여, 실험참가자와 에이전트는 서로 대화를 주고 받을 수 있고 음식을 만드는 환경에서 에이전트가 요리 레시피를 제시하는 실험을 진행하였다. 실험에 대한 평가 지표로는 embodiment and social eye-gaze 이다. 이후 Quantitative 평가방법으로, co-presence, gaze to agent, conversational turns를 측정하여, 모든 측면에서 SS &lt; AMR &lt; AMSR 순위로 결과가 나타났다. 그리고 에이전트에 intelligent 를 설문하였을때는 Anthropomorphism과 non-verbal cues 관계 없이 모두 동일한 intelligent 를 느꼈다고 답하였다. 반면 Qualitative 평가를 실시한 결과에는 주제에 따라 오히려 social cues가 없을때 더 선호된다는 결론을 얻었다.<br /></p>

<p><strong>장단점</strong><br />
인공지능 에이전트 사용에 있어서 현실에서 고려할만한 경우들에 대하여 가설검증을 통해 확인한 점이 장점이라고 생각한다. 또한 실험 후 qualitative measures를 통해 실험 참가자들의 의견을 확인 할 수 있어서 좋았다. 그러나 요리하는 한가지 상황에 대해서만 측정하여 다른 환경에서는 어떤 결과가 나올지 궁금한 측면이 있다.<br /></p>

<p><strong>의의</strong><br />
인공지능 에이전트와 소통하는 과정에서 non-verbal cues의 영향을 확인할 수 있었으며, 학습 주제별로 적절한 사용이 필요함을 확인할 수 있었다.<br /></p>

<p><br /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Kontogiorgos, D., Pereira, A., Andersson, O., Koivisto, M., Rabal, E. G., Vartiainen, V., &amp; Gustafson, J. (2019). The effects of anthropomorphism and non-verbal social behaviour in virtual assistants. IVA 2019 - Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents, 133–140.
</code></pre></div></div>

<p><br /></p>



</div>

<div class="pagination">
  
    <a href="/blog/2021-02-22/study" class="left arrow">&#8592;</a>
  
  
    <a href="/blog/2021-02-08/study" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>

    <footer>
  <span>
    &copy; <time datetime="2022-12-20 15:33:14 +0900">2022</time> Yoon Kyung Lee. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme.
  </span>
</footer>

  </body>
</html>
