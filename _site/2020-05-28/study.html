<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>May 28st Journal Club | Research Blog</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="May 28st Journal Club" />
<meta name="author" content="Yoon Kyung Lee (yoonlee78@snu.ac.kr)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Themes: NLP, Virtual Communication, Social Presence, Microagression, Dataset, KakaoTalk, Respiration, HRV, Emotion" />
<meta property="og:description" content="Themes: NLP, Virtual Communication, Social Presence, Microagression, Dataset, KakaoTalk, Respiration, HRV, Emotion" />
<link rel="canonical" href="http://localhost:4000/blog/2020-05-28/study" />
<meta property="og:url" content="http://localhost:4000/blog/2020-05-28/study" />
<meta property="og:site_name" content="Research Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-28T00:00:00+09:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="May 28st Journal Club" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Yoon Kyung Lee (yoonlee78@snu.ac.kr)"},"dateModified":"2020-05-28T00:00:00+09:00","datePublished":"2020-05-28T00:00:00+09:00","description":"Themes: NLP, Virtual Communication, Social Presence, Microagression, Dataset, KakaoTalk, Respiration, HRV, Emotion","headline":"May 28st Journal Club","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/2020-05-28/study"},"url":"http://localhost:4000/blog/2020-05-28/study"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/assets/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Research Blog" />

  <!-- Google Analytics-->
  
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/blog/">
      <h2 class="nav-title">Research Blog</h2>
    </a>
    <ul>
      <li><a href="/blog/about">About</a></li>
      <li><a href="/blog/">Posts</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        Yoon Kyung Lee (yoonlee78@snu.ac.kr)
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2020-05-28 00:00:00 +0900">May 28, 2020</time>
    
  </div>

  <h1 class="post-title">May 28st Journal Club</h1>
  <div class="post-line"></div>

  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Themes: NLP, Virtual Communication, Social Presence, Microagression, Dataset, KakaoTalk, Respiration, HRV, Emotion
</code></pre></div></div>

<p>Presenters: YK(Yoon Kyung), YW(Yoonwon), JE(Jaeun), IJ(Injoo), HY(Hoyoung)</p>

<hr />
<p><br /></p>

<h2 id="topic-1-natural-language-processing-with-spacy-introduction-programming-mini-workshopyk">Topic 1: Natural Language Processing with SpaCY Introduction (Programming Mini-Workshop)[YK]</h2>

<p>자연어처리 분석 및 모델링에 필요한 파이썬 라이브러리, SpaCY소개 및 실습 시간을 가졌다. SpaCY는 NLTK 를 곧 대체할 수 있는 차세대 라이브러리로 부상하고 있다! 한국어 형태소 분석은 Mecab을 dependency로 사용해서, 한국어 성능도 좋은 것을 확인했다.</p>

<p>링크: spacy.io</p>

<h2 id="topic-2-effect-of-conversational-agent-with-voice-and-face-in-older-adults-loneliness-ywj">Topic 2: Effect of Conversational Agent with Voice and Face in older adult’s loneliness [YWJ]</h2>

<p>선정 이유: 인공지능과의 대화에 있어서도 목소리만 있는 음성 비서인지 얼굴을 보며 대화할 수 있는 인공지능인지의 차이가 외로움 감소 효과에 차이를 가져오는지 궁금하였다. 이 연구가 노인 집단을 대상으로 얼굴을 보며 대화할 수 있는 인공지능이 외로움에 미치는 영향을 탐구하여 선정하게 되었다. 또한, 인공지능 프로그램을 디자인할 때 대화 주제와 대화 응답 패턴을 어떻게 선정했는지 설명되어 있어, 대화 시나리오 선정에 참고하였다.</p>

<p>내용 요약: 사회적으로 고립된 독거노인들을 대상으로 대화가 가능한 인공지능을 노인들의 집 컴퓨터에 설치하여 연구를 진행하였다. 인공지능은 합성된 목소리와 얼굴 및 상반신을 가지고 있고, 대화의 과정에서 손동작, 머리 끄덕임, 자세 바뀜 등의 비언어적 행동을 사용할 수 있었다.  대화 내용은 크게 두 가지로 미리 스크립트가 작성되어 그에 따라 진행되었다: (1) Companionship and social support (2) Loneliness and depressive symptoms interventions. 일주일 동안 노인들이 인공지능과 자유롭게 대화를 나누도록 한 결과,  생활 환경 안에서 노인들에게 먼저 말을 거는 virtual agent(proactive agent)와 대화한 경우가 노인들이 먼저 말을 걸어야 응답하는 virtual agent(passive agent)와 대화한 경우보다 외로움이 더 감소했고, 행복감과 만족감 그리고 편안함은 더 증가했다.</p>

<p>단점: 구현된 인공지능이 음성 인식을 기반으로 한 것이 아니라서, 인공지능은 노인들에게 말을 걸어도 노인들은 컴퓨터에 제시된 답변 후보 중에 하나를 객관식으로 선택하여 소통할 수밖에 없었다는 점이 실제 대화 경험을 제공하지 못하였다.</p>

<p>의의: 노인 집단을 대상으로 인공지능이 외로움 감소에 가질 수 있는 효과를 탐구하였다. 능동적인 인공지능과 수동적인 인공지능을 구분하여 그 효과의 차이를 살펴보았다. 이러한 요소들은 관련 연구의 디자인뿐만 아니라 실제적인 서비스 디자인에도 중요하게 작용할 것이다.</p>

<p>Ring, L., Barry, B., Totzke, K., &amp; Bickmore, T. (2013, September). Addressing loneliness and isolation in older adults: Proactive affective agents provide better support. <em>In 2013 Humaine Association Conference on Affective Computing and Intelligent Interaction (pp. 61-66). IEEE</em>.</p>

<hr />
<p><br /></p>

<h2 id="topic-3-ambiguous-chat-message-mood-detection-ij">Topic 3: Ambiguous chat message, mood detection [IJ]</h2>

<p>선정 이유: 채팅 상황에서의 감정 파악과 관련된 연구이기 때문에 개인 연구에 참고할 부분이 있을까 하여 읽어 보았다.</p>

<p>내용 요약:본 연구에서 ambiguous한 대화는 sentence valence(positive/negative)와 emoticon valence(positive/negative)가 incongruent한 경우를 의미한다. ambiguous한 대화에서 상대방의 감정을 파악할 때, emoticon의 영향이 강한지 아니면 sentence valence와 emoticon valence가 모두 영향을 주는지 확인하고자 하였다. 그 결과, ambiguous한 대화의 경우 emoticon valence와 상관없이 상대방이 negative mood일 것이라고 판단하는 경향을 보였다. 또한 emoticon valence가 positive할 때 이러한 경향이 더 강하게 나타났다.</p>

<p>장단점
장점: 채팅 대화 상황에서 상대방의 감정을 지각할 때 sentence valence와 emoticon valence를 모두 고려한다는 것을 보여주었다.
단점: 실제 채팅 대화 상황에서 실험을 진행한 것이 아니라 PsychoPy를 통해 자극을 했다는 점이 아쉽다.</p>

<p>의의: 사람들이 채팅 대화 상황에서 상대방의 감정을 파악할 때 emoticon만으로 감정을 파악하지 않고, emoticon과 sentence valence 간의 관계를 고려하여 파악한다는 것을 보여주었다.</p>

<p>비고: sentence과 emoticon을 크게 positive / negative로 간단하게 분류했다는 점, 각 sentence과 emoticon valence의 intensity를 통제하기 위해 사전에 실시한 validation 과정 등을 참고할 수 있을 것 같다.</p>

<p>Aldunate, N., Villena-González, M., Rojas-Thomas, F., López, V., &amp; Bosman, C. A. (2018). Mood detection in ambiguous messages: the interaction between text and emoticons. <em>Frontiers in psychology, 9</em>, 423.</p>

<hr />
<p><br /></p>

<h2 id="topic-4-video-hy">Topic 4: Video [HY]</h2>

<p>선정 이유: Video 내에서 어떻게 사람의 행동을 분석하는지 알고 싶었고, 과거 부터 현재 어떻게 진행되고 있는지 확인하고 싶어 읽어보았음.</p>

<p>내용 요약: 최근 사람들을 묘사하는 비디오가 많아지고 있는데, 비디오 안에서 일어나고 있는 사람과 사람간의 상호작용에 대해 자동으로 얼마나 잘 분석해주는지에 대한 연구가 많이 진행되고 있고, 대부분 챌리지한 도전이 대부분이다. 과거에는 BoW or Fisher vector(FV), Dense Trajectories(DT) 등이 사용되었고 Template-based approaches이 기본적이였다. 최근에는 Motion-based and stream networks 접근으로 분석을 하는 것이 기본적이다. CNN -&gt; RNN -&gt; LSTM 으로 발전되어 나가고 있다. 최신 기술의 한계는 데이터가 쏟아져 나오는데 , 이에 상응하는 train data가 부족하다. 따라서 좀더 광범위하게 사용될 수 있는 train data를 적용시키는 노력이 필요하다.</p>

<p>장단점
장점 : 현재 연구되고 있는 전체 충분한 dataset에 대해 알 수 있었다.과거에는 정지되어 있는 물체를 확인하는 것에서, 최근에 활용되고 있는 딥러닝 기술을 알 수 있음. 각각의 방법에 따라 어떤 연구가 진행되어 있는지 확인할 수 있었다.
단점 : 실제 분석 하는 방법에 대한 프로세스 혹은 기법 설명 예시가 있으면 좋을 것 같다. 전문용어들이 많이 나오다 보니 이해하기 다소 어렵다.</p>

<p>의의: Video 내에 인간의 상호작용을 확인하는 방법들이 발전하면서, 어떠한 한계점이 있는지 확인할 수 있었다. 다만, 한계점을 해결하기 위해는 데이터 기반 기술 발전도 중요하지만, 궁극적으로 인간의 상호작용에 대한 이해를 하는 연구도 함께 진행되어야 할 것이다.</p>

<p>Stergiou, A., &amp; Poppe, R. (2019). Analyzing human–human interactions: A survey. <em>Computer Vision and Image Understanding, 188</em>, 102799</p>

<hr />
<p><br /></p>

<h2 id="topic-5-emotion-recognition-facial-expression-context-in-emotion-recognition-je">Topic 5: Emotion Recognition; Facial Expression; Context in Emotion Recognition [JE]</h2>

<p>선정 이유: 얼굴표정과 주변 맥락, 감정인식의 관계에 대해 더 알아보고 싶어서</p>

<p>요약
-자연스러운 상황에서의 감정 표현의 경우 얼굴 표정만으로 정확한 감정을 인식하기 어려우며 몸짓 등 주변 맥락이 있어야 정확한 인식이 가능하다.
-stereotypical한 얼굴표정의 경우에도 맥락이 어떻게 주어지느냐에 따라 anger와 disgust 등의 구분이 어렵다
-감정인식에 있어 맥락의 유무가 인식 과정 자체에 변화를 준다. 
-body language 이외에도 감정표현을 하는 사람 외부 요인 (상호작용 상황, 언어적 정보 등)의 영향도 있음</p>

<p>장단점
장점: 리뷰 논문으로 맥락이 감정 인식에 영향을 준 다양한 연구 결과에 대해 읽을 수 있었음
단점: 자연스러운 상황에서의 감정인식에 대한 내용은 많지 않아 아쉬웠음.</p>

<p>의의
얼굴 표정 인식에 있어서 주변 맥락의 중요성을 보여주는 연구 결과를 잘 정리함.</p>

<p>Aviezer, H., Ensenberg, N., &amp; Hassin, R. R. (2017). The inherently contextualized nature of facial emotion perception. <em>Current Opinion in Psychology, 17</em>, 47-54.</p>



</div>

<div class="pagination">
  
    <a href="/blog/2020-06-04/study" class="left arrow">&#8592;</a>
  
  
    <a href="/blog/2020-05-21/study" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>

    <footer>
  <span>
    &copy; <time datetime="2022-12-20 15:33:14 +0900">2022</time> Yoon Kyung Lee. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme.
  </span>
</footer>

  </body>
</html>
