---
layout: post
title: "Jul 12th Journal Club"
author: "Joohye Lee (joohyelee@snu.ac.kr)"
---

    Themes:Human–AI collaboration, Empathic Conversation, Peer Support, Learning by Asking

Presenters: HL(Hyunjoo Lee), CL(Chunghyun Lee) <br>

-----------------


# Topic 1: Testing theory of mind in large language models and humans [HL]

### **선정 이유**

llm이 사람의 인지과정 중 중요한 요소인 theory-of-mind(ToM) 능력을 얼마만큼 가지고 있는지 아는 것은 더 고차원적인(사람 같은) 추론과 대화를 하는 llm을 개발하는 데 도움이 될 것. systematic experiment이 인상깊었음

### **내용 요약**

- false belief, irony, faux pas, hinting, strange stories 테스트
- faux pas 제외, GPT-4이 사람보다 높은 정확도를 보여줌
- 유일하게 faux pas에서 GPT-4가 사람보다 낮은 퍼포먼스를 보인 반면, LLaMa2는 유일하게 faux pas에서 사람보다 높은 정확도를 보임
- faux pas 결과를 더 자세히 들여다보기 위한 추가 실험: faux pas likelihood test, belief likelihood test


### **한계점(선택)**

2-3문장 내외 (선행 연구와 비교, 이해가 어려운 부분 등)

### **의의**

단순 ToM 태스크 수행과 결과로는 알아낼 수 없는 모델별 특성에 따른 태스트 수행능력을 분석함, 더욱 고차원적인 실험 설계의 필요성을 짚어냄
5-6문장 내외 (학술적, 사회적 기여)

### **비고**

참고 자료 소개 (해당시), 추가 코멘트(해당시)

### **참고 문헌**

Strachan, J.W.A., Albergo, D., Borghini, G. et al. Testing theory of mind in large language models and humans. Nat Hum Behav (2024). 



# Topic 2: Detecting hallucinations in large language models using semantic entropy [CL]


# Keywords: Hallucinations, LLM, semantic entropy


### **선정 이유**

첫 스터디때 Hallucinations라는 개념을 처음 접하며 흥미가 생겨 알아보게 되었다.

### **내용 요약**

ChatGPT와 같은 LLM은 종종 '환각 (Hallucinations)'이라 불리는 잘못된 정보나 근거 없는 답변을 생성하는데, 이는 법적, 의료적, 뉴스 보도 등 다양한 분야에서 심각한 문제를 초래할 수 있음. 진실성을 강화하기 위한 기존의 감독 및 강화 학습 방식은 부분적으로만 성공적이어서 새로운 환각 탐지 방법이 필요함.

본 연구는 의미적 엔트로피(semantic entropy)를 기반으로 불확실성을 추정하여 LLM의 환각, 특히 '허구(confabulations)'를 탐지하는 방법을 제안함. 이 방법은 단어들이 아닌 의미 수준에서의 불확실성을 계산하여 잘못되고 임의적인 생성을 감지함. 이를 위해 여러 가능한 답변을 샘플링하고 이 답변들을 의미적으로 유사한 클러스터로 묶은 다음, 각 클러스터의 불확실성을 계산함.

그 결과, TriviaQA, SQuAD, BioASQ 등 여러 데이터셋과 도메인에서 환각을 효과적으로 탐지함. 또한, LLaMA 2 Chat, Falcon Instruct, Mistral Instruct 등, 여러 모델에서 의미적 엔트로피가 다른 방법들보다 환각을 더 잘 탐지함. 의미적 엔트로피는 질문의 배경지식이 없는 상황에서도 잘 작동한다는 장점도 가짐.



### **의의**

의미적 엔트로피는 LLM의 지식 부족으로 인한 환각을 효과적으로 탐지하며 질문에 대한 답변 뿐만 아니라 요약 등, 다양한 도메인에서도 유용하게 사용될 수 있고 LLM의 신뢰성을 높이는 데 중요한 도구가 될 수 있음.

### **비고**

N/A

### **참고 문헌**

Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017), 625-630.



# Topic 3: Investigating the effect of Top-Down Processing feedback to enhance Perceived Control when riding Mobility Assisting Robots (연구계획 발표) [JL]


