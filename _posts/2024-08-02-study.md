---
layout: post
title: "Aug 2nd Journal Club"
author: "Joohye Lee (joohyelee@snu.ac.kr)"
---

    Themes:Human–AI collaboration, Empathic Conversation, Peer Support

Presenters: SL(Soryoo Lee), GK(Gyuyi Kang), SO(Serin Oh) <br>

-----------------


# Topic 1: The potential and limitations of large language models in identification of the states of motivations for facilitating health behavior change [SL]


### **선정 이유**

5-6문장 내외

### **내용 요약**

이 논문은 대형 언어 모델(LLM) 기반 생성 대화형 에이전트(GA)가 건강 행동 변화의 다양한 동기 상태를 인식하고 적절한 정보를 제공하는 능력을 평가한다. 
연구는 ChatGPT, Google Bard, Llama 2를 사용하여 변화단계모델(TTM)의 다섯 가지 단계(전숙고, 숙고, 준비, 실행, 유지)에 걸친 25개의 시나리오를 통해 GA의 성능을 평가했다.
결과적으로 GA는 준비 단계에서 동기 상태를 잘 인식하고 행동 단계로 나아가기 위한 충분한 정보를 제공했으나, 전숙고 및 숙고 단계에서는 제한된 정보만 제공했습니다.
이 연구는 GA가 명확한 목표와 결심을 가진 사용자에게는 효과적인 지원을 제공할 수 있지만, 행동 변화에 대해 양가적인 사용자에게는 불충분할 수 있음을 시사한다.

### **한계점(선택)**
연구가 공개적으로 접근 가능한 가장 널리 사용되는 GA를 대상으로 했기 때문에, 개인화된 또는 전문화된 GA의 성능을 평가하지 못했다는 점이 있다. 
또한, 선행 연구들과 달리, 이 연구는 사용자의 잠재적 동기 상태를 인식하는 데 있어 GA의 한계를 명확히 드러냈다.
이는 향후 연구에서 GA의 편향성과 사용자 동기 상태 인식 개선을 위한 추가적인 평가가 필요함을 시사한다.

### **의의**

대형 언어 모델(LLM) 기반 대화형 에이전트(GA)가 건강 행동 변화의 다양한 동기 상태를 인식하고 적절한 정보를 제공하는 데 있어 한계를 명확히 밝혔다는 점에서 학술적 의의가 있다.
이는 LLM의 잠재력과 한계를 이해하고, 보다 정교한 동기 상태 인식 및 맞춤형 정보 제공을 위한 연구와 개발 방향을 제시한다.
사회적으로는 GA가 건강 행동 변화 촉진에 기여할 수 있는 방법을 모색함으로써, 공공 건강 증진과 개인 맞춤형 건강 관리에 기여할 수 있다. 
이 연구는 향후 GA 기술을 개선하고, 더 나은 건강 정보 제공 시스템을 구축하는 데 중요한 기초 자료를 제공한다.

### **비고**


### **참고 문헌**

Bak, M., & Chin, J. (2024). The potential and limitations of large language models 
in identification of the states of motivations for facilitating health behavior change. 
Journal of the American Medical Informatics Association, ocae057.



# Topic 2: Failing to Learn and Learning to Fail (Intelligently): How Great Organizations Put Failure to Work to Innovate and Improve [GK]

### **선정 이유**

실패의 원인을 분석하고 개선할 수 있는 기회를 제공함으로써 효과적으로 학습 할 수 있기때문에 선정하였다. 

### **내용 요약**

이 연구는 실패를 효과적으로 분석하고 학습하기 위한 방법을 탐구하였다. 조직 내에서 실패를 투명하게 공유하고 분석할 수 있는 문화와 시스템의 필요성을 강조하며 다양한 조직 사례를 통해 실패 분석의 중요성을 입증하고, 실패를 분석하기 위한 구체적인 절차와 도구를 제안하였다. 

### **한계점(선택)**

N/A

### **의의**

실패를 통해 배워야하지만 실천하지 못하는 경우가 많다. 이 연구는 학술적으로 실패를 학습의 중요한 요소로 분석하고 실패를 분석하기 위한 구체적인 절차를 제시하였다. 
### **비고**

참고 자료 소개 (해당시), 추가 코멘트(해당시)

### **참고 문헌**
Cannon, M. D., & Edmondson, A. C. (2005). Failing to learn and learning to fail (intelligently): How great organizations put failure to work to innovate and improve. Long range planning, 38(3), 299-319.


# Topic 3: Dissociating Language and Thought in Large Language Models [SO]

### **선정 이유**

LLM의 활용이 주요한 과제로 떠오르고 있는 지금, LLM의 성능을 크게 두 분야로 나누어 각 성취 수준을 탐색함으로써 LLM의 현재 위치를 개괄적으로 훑어볼 수 있는 논문으로 보여 선정하였다. 실험보다는 리뷰 논문에 가까워 레퍼런스 또한 많은 까닭에, 흥미로운 세부 사항이 있다면 해당 연구를 찾기에도 용이해 보인다. 

### **내용 요약**

본 논문에 따르면, LLM의 성능은 1) 음운론, 형태론, 통사론 등 linguistic form을 배우는 formal linguistic competence와 2) 주변 상황을 묘사하는 등 사고 능력과 관련된 functional linguistic competence로 구분해 평가하여야 한다.
최근 뇌 과학에서 사람 또한 언어 능력과 사고 능력을 사용할 때 활성화되는 뇌 영역이 확연히 다르다고 보고하는 만큼, LLM이 인간의 사고 과정을 얼마나 따라잡았는지 파악하기 위해 이처럼 이분된 평가 기준을 적용하는 일은 타당하다.
각 기준에 대해, LLM은 적어도 영어에 대해서는 기존의 통계적 모형을 능가해 사람과 유사한 수준의 formal competence를 보인다. LLM은 외현적인 언어 구조를 파악하는 것은 물론이고 filler-gap dependency 등 여러 언어 현상을 이해한다. 이뿐 아니라, 긴 문장 안의 복잡한 위계 구조와 품사 등 언어 내적으로 추상화된 개념, 통사적 구조를 알아 이를 따르는 문장을 무리 없이 도출한다.
그러나 LLM은 functional competence 측면에서 여전히 부족한 결과를 보인다. 이를테면, formal reasoning task에서 chain of thought 접근을 지시해도 완벽한 답안을 보장하지 못하며, 상식 층위의 정보 처리에서는 hallucination 등 결함을 보인다. 또한, 주어진 정보를 표상하고 이를 기존의 것과 통합하는 situation modeling에도 어려움을 보이며, theory of mind나 화용론 등 정보의 사회적인 맥락을 읽어내는 일에도 아직 미흡하다.
이를 개선하기 위해, 본 논문에서는 인간의 뇌가 formal competence와 functional competence를 분담하여 작동하듯이 LLM 또한 modular 개념을 도입할 것을 제안한다.

### **의의**

현재 LLM의 발전과 한계를 명확하게 짚어주어 연구자들이 어떠한 지점에서 LLM 연구를 지속하거나 시작해야 할지 판단하는 데 도움이 될 것으로 예상한다.
다만, 해당 논문이 2024년 6월에 나왔음에도 불구하고 그사이 GPT4가 theory of mind 과제에서 이전 모델들과 다르게 인간에 버금가는 결과를 보였다고 보고한 논문이 저널에 실린 점을 고려하자면, 해당 논문을 참고하되 빠르게 발전하는 기술의 가장 최근 동향을 파악하는 일도 중요해 보인다.

### **참고 문헌**

Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko, E. (2024). Dissociating language and thought in large language models. Trends in Cognitive Sciences, 28(6), 517-540.






